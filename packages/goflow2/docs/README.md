# GoFlow2

The GoFlow2 integration allows you to import logs generated by goflow2.

The only supported protocol is sflow, since there are already existing integrations for netflow/IPFIX.

## Data streams
### sflow
The Goflow2 sFlow integration collects one type of data streams: logs

#### Sample Event
An example event for `sflow` looks as following:

```json
{
    "agent": {
        "name": "slvimssflowmetertest01",
        "id": "7641289c-925d-4314-b8ff-586470e05333",
        "type": "filebeat",
        "ephemeral_id": "6d1b8c3f-5f2a-4296-903c-7a595f114eca",
        "version": "8.14.2"
    },
    "log": {
        "file": {
            "path": "/var/log/sflow/goflow2/goflow2.log"
        },
        "offset": 849482434
    },
    "elastic_agent": {
        "id": "7641289c-925d-4314-b8ff-586470e05333",
        "version": "8.14.2",
        "snapshot": false
    },
    "destination": {
        "geo": {
            "continent_name": "Europe",
            "region_iso_code": "DE-BY",
            "city_name": "Munich",
            "country_iso_code": "DE",
            "country_name": "Germany",
            "location": {
                "lon": 11.5658,
                "lat": 48.1336
            },
            "region_name": "Bavaria"
        },
        "as": {
            "number": 8767,
            "organization": {
                "name": "M-net Telekommunikations GmbH"
            }
        },
        "address": [
            "88.217.238.90"
        ],
        "port": 0,
        "ip": "88.217.238.90"
    },
    "source": {
        "geo": {
            "continent_name": "Europe",
            "region_iso_code": "DE-NW",
            "city_name": "Cologne",
            "country_iso_code": "DE",
            "country_name": "Germany",
            "location": {
                "lon": 6.9909,
                "lat": 50.8806
            },
            "region_name": "North Rhine-Westphalia"
        },
        "as": {
            "number": 3320,
            "organization": {
                "name": "Deutsche Telekom AG"
            }
        },
        "address": [
            "195.243.188.74"
        ],
        "port": 0,
        "ip": "195.243.188.74"
    },
    "network": {
        "bytes": 1482000,
        "transport": "esp",
        "type": "ipv4",
        "packets": 1000
    },
    "observer": {
        "ingress": {
            "vlan": {
                "id": 1500
            },
            "interface": {
                "id": 563
            }
        },
        "ip": [
            "10.13.38.90"
        ],
        "egress": {
            "interface": {
                "id": 573
            }
        },
        "egress": {
            "vlan": {
                "id": 1500
            }
        }
    },
    "ecs": {
        "version": "8.11.0"
    },
    "related": {
        "ip": [
            "195.243.188.74",
            "88.217.238.90"
        ]
    },
    "event": {
        "agent_id_status": "auth_metadata_missing",
        "ingested": "2024-07-31T12:08:39Z",
        "original": "{\"type\":\"SFLOW_5\",\"time_flow_start_ns\":1722427718326647366,\"sampler_address\":\"10.13.38.90\",\"sequence_num\":49724,\"in_if\":563,\"out_if\":573,\"src_addr\":\"195.243.188.74\",\"dst_addr\":\"88.217.238.90\",\"etype\":\"IPv4\",\"proto\":\"ESP\",\"src_port\":0,\"dst_port\":0,\"src_vlan\":1500,\"dst_vlan\":1500,\"sampling_rate\":1000,\"bytes\":1482}",
        "timezone": "+00:00",
        "kind": "event",
        "action": "SFLOW_5",
        "category": [
            "network"
        ],
        "type": [
            "connection"
        ],
        "dataset": "goflow2.sflow"
    },
    "sflow": {
        "sequence_num": 49724,
        "sample_rate": 1000,
        "bytes": 1482
    },
    "tags": [
        "sflow",
        "forwarded",
        "preserve_original_event"
    ],
    "input": {
        "type": "log"
    },
    "@timestamp": "2024-07-31T12:08:38.000Z",
    "data_stream": {
        "namespace": "default",
        "type": "logs",
        "dataset": "goflow2.sflow"
    }
}

```

## Requirements

You need Elasticsearch for storing and searching your data and Kibana for visualizing and managing it.
You can use our hosted Elasticsearch Service on Elastic Cloud, which is recommended, or self-manage the Elastic Stack on your own hardware.

You need GoFlow2 to create log files for sFlow traffic.
https://github.com/netsampler/goflow2

## Setup

- Install integration and role out elastic agent
- Install GoFlow2 for sFlow logging

Please use the following GoFlow2 mapping.yaml file:

```
# File: /etc/goflow2/mapping.yaml
formatter:
    fields: # list of fields to format in JSON
        - type
        - time_flow_start_ns
        - sampler_address
        - sequence_num
        - in_if
        - out_if
        - src_addr
        - dst_addr
        - etype
        - proto
        - src_port
        - dst_port
        - src_vlan
        - dst_vlan
        - sampling_rate
        - bytes
```

The output sflow transport files must be stored in the directory ```/var/log/sflow/goflow2/```

Full command to run GoFlow2 for sflow traffic:
```shell
goflow2 -format json -listen "sflow://:6343" -mapping /etc/goflow2/mapping.yaml -transport.file /var/log/sflow/goflow2/goflow2.log
```

## Fields
**Exported fields**

| Field | Description | Type |
|---|---|---|
| @timestamp | Event timestamp. | date |
| data_stream.dataset | Data stream dataset. | constant_keyword |
| data_stream.namespace | Data stream namespace. | constant_keyword |
| data_stream.type | Data stream type. | constant_keyword |
| input.type | input type | constant_keyword |
| log.offset | logfile offset | long |
| sflow.bytes | Original Bytes of the sample package. | long |
| sflow.sample_rate | sflow sample rate. | long |
| sflow.sequence_num | sflow sequence number. | long |

